\section{Implementación del \textit{pipeline}}
El desarrollo y la evaluación de los modelos predictivos se realizaron íntegramente en \textbf{Python}, utilizando librerías como \texttt{scikit-learn}, \texttt{cuML} y \texttt{PyTorch}, junto con implementaciones específicas de \textit{gradient boosting} como \texttt{XGBoost}, \texttt{LightGBM} y \texttt{CatBoost}. El proceso de entrenamiento se llevó a cabo en dos fases diferenciadas. 

Para los modelos entrenados exclusivamente con datos del IFN3 se utilizó un equipo local equipado con un procesador Intel Core i7 y 32~GB de memoria RAM. En cambio, los modelos que empleaban conjuntamente datos del IFN2 e IFN3 se entrenaron en el sistema de computación de alto rendimiento (HPC) de la Universidad de Salamanca. Esta elección se debió a la disponibilidad de tarjetas gráficas Nvidia H100, que permiten acelerar de forma significativa el entrenamiento de aquellos modelos compatibles con ejecución en GPU gracias a su elevada capacidad de paralelización. 

No obstante, cabe señalar que el entrenamiento también podría haberse realizado en un equipo de escritorio convencional equipado con una tarjeta gráfica comercial, ya que los requisitos computacionales del problema no son especialmente elevados.

\subsection{Ingeniería práctica del entrenamiento y la validación}

Desde el punto de vista de la implementación, el proceso de entrenamiento y validación se apoyó fundamentalmente en el ecosistema de \texttt{scikit-learn}, complementado con librerías especializadas para modelos de \textit{gradient boosting}. La gestión de los datos se realizó mediante \texttt{pandas} y \texttt{numpy}, mientras que el cálculo de métricas y estadísticas adicionales del error se apoyó en \texttt{scipy} y los módulos de evaluación de \texttt{sklearn.metrics}.

A partir del conjunto de datos original, se aplicaron filtros de calidad sobre la variable objetivo utilizando operaciones vectorizadas de \texttt{pandas}, eliminando observaciones con valores nulos, inconsistentes o que no cumplían los criterios definidos en la Sección~\ref{subsec:filtradoregistros}. 

La partición de los datos en conjuntos de entrenamiento y prueba se realizó mediante \texttt{GroupShuffleSplit} del módulo \texttt{sklearn.model\_selection}, con una proporción 80/20. Este esquema garantizó que todas las observaciones asociadas a una misma parcela se asignaran íntegramente a un único subconjunto, evitando fugas de información derivadas de la correlación espacial intra-parcela. Sobre el conjunto de entrenamiento se definió una validación cruzada de cinco pliegues utilizando \texttt{GroupKFold}.

El preprocesado de las variables y el ajuste de los modelos se integraron en un único objeto \texttt{Pipeline}, combinando \texttt{ColumnTransformer}, \texttt{SimpleImputer}, \texttt{StandardScaler} y \texttt{OneHotEncoder}. Esta integración aseguró que todas las transformaciones se estimaran exclusivamente con los datos de entrenamiento de cada pliegue durante la validación cruzada. El ajuste de hiperparámetros se llevó a cabo mediante \texttt{GridSearchCV}, definiendo rejillas específicas para cada algoritmo y utilizando el coeficiente de determinación ($R^2$) como métrica de selección.

Los modelos evaluados incluyen implementaciones de \textit{gradient boosting} (\texttt{XGBoost}, \texttt{LightGBM}, \texttt{CatBoost} y \texttt{GradientBoostingRegressor}), métodos basados en \textit{bagging} (\texttt{RandomForestRegressor}, \texttt{BaggingRegressor}), así como modelos de distinta naturaleza como \texttt{MLPRegressor}, \texttt{KNeighborsRegressor}, \texttt{LinearSVR}, \texttt{AdaBoostRegressor} y \texttt{BayesianRidge}. Para cada modelo se calcularon de forma sistemática las métricas de rendimiento sobre el conjunto de prueba: $R^2$, RMSE y MAE, junto con estadísticas adicionales del error absoluto (mediana y moda), almacenándose los resultados en estructuras tabulares para su análisis comparativo.

\subsection{Implementación del \textit{stacking}}

La agregación de modelos mediante \textit{stacking} se implementó de forma manual utilizando utilidades básicas de \texttt{scikit-learn}, con el objetivo de mantener un control estricto sobre el flujo de entrenamiento y validación. A partir de los mejores modelos individuales se generaron predicciones fuera de pliegue (\textit{out-of-fold}, OOF) sobre el conjunto de entrenamiento, empleando el mismo esquema de validación cruzada (\texttt{GroupKFold}).

Estas predicciones OOF se organizaron en matrices de meta-variables mediante \texttt{numpy} y se utilizaron como entrada para el entrenamiento de los metamodelos. En paralelo, cada modelo base se reentrenó sobre la totalidad del conjunto de entrenamiento para generar las correspondientes predicciones sobre el conjunto de test, que se emplearon posteriormente para la evaluación final del \textit{stack}.

Los metamodelos considerados incluyen \texttt{LinearRegression}, \texttt{Ridge}, \\
\texttt{GradientBoostingRegressor}, \texttt{RandomForestRegressor}, \texttt{SVR} con kernel lineal y \texttt{MLPRegressor}. Antes de su ajuste, las meta-variables se estandarizaron mediante \texttt{StandardScaler}, integrando este paso en un \texttt{Pipeline} específico del segundo nivel. La evaluación del \textit{stacking} se realizó exclusivamente sobre el conjunto de test independiente, calculando las métricas habituales ($R^2$, RMSE y MAE) para cada combinación de modelos base y metamodelo.


\subsection{Datos finales de entrenamiento}
\label{subsec:datos_finales_entrenamiento}

Tras aplicar los criterios de elegibilidad y filtrado descritos en la
Sección~\ref{subsec:filtrado_registros}, el conjunto de datos final utilizado
para el ajuste de los modelos queda compuesto por:

\begin{itemize}
    \item \textbf{IFN2:} Total de parcelas = \textbf{88.696}
          \begin{itemize}
              \item Casos con $c4 > c$: \textbf{31.428}
              \item Casos con $carbono\_bruto4 > carbono\_bruto$: \textbf{32.403}
          \end{itemize}

    \item \textbf{IFN3:} Total de parcelas = \textbf{171.157}
          \begin{itemize}
              \item Casos con $fccarb > 20$: \textbf{158.434}
              \item Casos con $fccarb > 20$ y $c4 > c$: \textbf{57.401}
              \item Casos con $fccarb > 20$ y $carbono\_bruto4 > carbono\_bruto$: \textbf{76.617}
          \end{itemize}
\end{itemize}

La Tabla~\ref{tab:resumen_variables} resume las principales estadísticas
descriptivas de las variables utilizadas en el modelado, adicionalmente en la Figura~\ref{fig:hist_c4} se muestra la distribución de las mismas.

\begin{table}[htbp]
    \centering
    \caption{Estadísticos descriptivos del conjunto de datos depurado.}
    \label{tab:resumen_variables}
    \begin{tabular}{lrrrrr}
        \toprule
        Variable        & N        & Media   & Desv.\ estándar & Mín.     & Máx.       \\
        \midrule
        carbono\_bruto4 & 136\,325 & 24.6168 & 35.8198         & 0.000327 & 420.498829 \\
        carbono\_bruto  & 114\,485 & 15.9326 & 26.3052         & 0        & 359.805707 \\
        c4              & 105\,714 & 38.3789 & 47.0348         & 0.484695 & 883.462735 \\
        c               & 92\,372  & 23.4399 & 34.9622         & 0        & 842.739088 \\
        periodo         & 105\,709 & 18.3167 & 6.4853          & 0        & 34         \\
        \bottomrule
    \end{tabular}
\end{table}

Se observa que la variable \texttt{carbono\_bruto4} presenta una media de 24.62 y una desviación estándar de 35.82, mientras que la variable
\texttt{c4} muestra valores notablemente superiores (media de 38.38 y desviación estándar de 47.03). La variable \texttt{c4} es más dispersa y heterogénea que \texttt{carbono\_bruto4}. En general, una mayor variabilidad en la variable objetivo se traduce en un problema de predicción más complejo, ya que el modelo debe capturar relaciones más inestables y sujetas a mayor ruido.

Por tanto, incluso antes de evaluar los modelos, es razonable esperar que una misma familia de algoritmos obtenga valores de $R^2$ más elevados y errores más bajos (RMSE, MAE) al predecir \texttt{carbono\_bruto4}, cuya estructura estadística es menos dispersa, que al predecir \texttt{c4}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{figuras/histograma_c.png}
    \caption{Distribución de las variables \texttt{c4} y \texttt{carbono\_bruto4} en el conjunto depurado.}
    \label{fig:hist_c4}
\end{figure}

Observamos una clara distribución asimétrica a la izquierda con una larga cola en ambas variables. No existe un factor de escala único que lleve de una variable a la otra porque la generalización a hectárea tiene en cuenta la densidad forestal particular de cada parcela.

\subsubsection{Efecto del periodo sobre el carbono}
\label{subsec:resultados_periodo_anova}

La influencia del \textit{periodo} sobre las variables de carbono se evaluó
mediante ANOVA de un factor. Los análisis realizados \
muestran que el \textit{periodo} ejerce un efecto significativo sobre ambas
variables. En \texttt{c4} se obtuvo un estadístico
\(F = 143.49\) \((p < 0.001)\), mientras que en \texttt{carbono\_bruto4}
el valor fue \(F = 161.08\) \((p < 0.001)\).
Estos resultados indican que las diferencias observadas entre periodos
no son aleatorias, sino que reflejan variaciones sistemáticas asociadas al
momento de muestreo, confirmando que el \textit{periodo} constituye un factor
explicativo relevante en la dinámica del carbono forestal.
