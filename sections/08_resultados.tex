\section{Resultados}
\label{sec:resultados}
% Aquí es donde analizas lo que significan tus resultados.
% - Interpretación de los resultados: ¿Qué significan los valores de tus métricas? ¿El modelo es bueno, aceptable, o tiene problemas?
% - Comparación con la literatura: ¿Cómo se comparan tus resultados con estudios similares en el campo? ¿Superas, igualas o quedas por debajo de lo esperado? ¿Por qué?
% - Implicaciones: ¿Qué implicaciones tienen tus hallazgos para la predicción de créditos de carbono, la gestión de dehesas, o la toma de decisiones empresariales?
% - Limitaciones del estudio: ¿Qué aspectos no pudo abordar tu modelo o tu metodología? (ej. tamaño del dataset, calidad de los datos, alcance geográfico, factores no considerados).
% - Áreas de mejora: ¿Cómo se podría mejorar el modelo o la investigación en el futuro? (ej. más datos, diferentes modelos de IA, considerar otras variables, integrar datos de otras fuentes).
% - Relevancia práctica: ¿Cómo puede ser utilizado este modelo en el mundo real?

%--------------------------------------------------

A continuación expondremos los resultados obtenidos a partir de varias métricas de evaluación de los modelos sobre el conjunto de datos de test. Dividiremos la sección según el origen de los datos de entrenamiento (IFN2, IFN3 o IFN2 e IFN3) y según la variable a predecir (toneladas de carbono o toneladas de carbono por hectárea).

\subsection{Resultados IFN2}

\subsubsection{Toneladas de carbono por hectárea}

\subsubsubsection{Modelos base}

Una vez entrenados los modelos, algunos parámetros globales como el $R^2$, RMSE y MAE se presentan en la Tabla \ref{tab:base_comp_ifn2_c4}.

\begin{table}[H]
    \centering
    \caption{Comparativa Modelos Base en IFN2 (c4)}
    \label{tab:base_comp_ifn2_c4}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lrrrrrr}
        \toprule
        Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC/ha)} & \thead{MAE$_{\text{test}}$ \\ (tC/ha)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC/ha)} \\
        \midrule
        \textbf{LightGBM$_{\text{global}}$} & \textbf{0.719} & \textbf{26.806} & \textbf{14.689} & \textbf{35.364} & \textbf{47.104} & \textbf{-0.021} \\
        LightGBM & 0.712 & 26.435 & 14.755 & 35.823 & 47.593 & -0.401 \\
        CatBoost$_{\text{global}}$ & 0.708 & 27.359 & 14.832 & 35.710 & 47.895 & -0.135 \\
        XGBoost$_{\text{global}}$ & 0.708 & 27.361 & 14.620 & 35.198 & 46.910 & -0.171 \\
        GBDT$_{\text{global}}$ & 0.705 & 27.504 & 14.772 & 35.564 & 46.941 & -0.142 \\
        XGBoost & 0.704 & 26.779 & 14.773 & 35.867 & 47.452 & -0.434 \\
        CatBoost & 0.701 & 26.927 & 15.066 & 36.578 & 49.284 & -0.753 \\
        MLP$_{\text{global}}$ & 0.686 & 28.335 & 15.668 & 37.722 & 50.547 & 0.280 \\
        BaggedDT & 0.676 & 28.036 & 15.683 & 38.075 & 49.869 & -0.195 \\
        GBDT & 0.673 & 28.161 & 15.614 & 37.907 & 50.513 & -0.699 \\
        RandomForest & 0.663 & 28.595 & 15.807 & 38.378 & 49.750 & -0.376 \\
        BaggedDT$_{\text{global}}$ & 0.658 & 29.574 & 15.933 & 38.361 & 50.974 & -0.599 \\
        RandomForest$_{\text{global}}$ & 0.653 & 29.803 & 15.577 & 37.502 & 48.386 & -1.499 \\
        MLP & 0.637 & 29.650 & 15.543 & 37.735 & 49.907 & -7.176 \\
        BayesianNN & 0.606 & 30.904 & 17.384 & 42.205 & 56.359 & -0.697 \\
        BayesianNN$_{\text{global}}$ & 0.588 & 32.482 & 17.725 & 42.675 & 56.630 & -0.732 \\
        KNN & 0.565 & 32.480 & 17.610 & 42.755 & 54.696 & -5.373 \\
        SVR$_{\text{global}}$ & 0.465 & 37.013 & 16.674 & 40.145 & 49.745 & -8.320 \\
        SVR & 0.445 & 36.667 & 17.361 & 42.150 & 52.112 & -8.338 \\
        AdaBoost & -2.126 & 87.062 & 81.700 & 198.354 & 118.313 & 78.210 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsubsubsection{Modelos con stacking}

En la Tabla \ref{tab:stack_comp_ifn2_c4} se muestra el resumen del rendimiento de los modelos de stacking para la predicción de la variable de carbono en tC/ha con el conjunto de datos que emplea IFN2 como explicativo.

{
{
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \begin{longtable}{p{4.5cm}rrrrrr}
    \caption{Comparativa Modelos Stacking en IFN2 (c4)}
    \label{tab:stack_comp_ifn2_c4} \\
    \toprule
    Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC/ha)} & \thead{MAE$_{\text{test}}$ \\ (tC/ha)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC/ha)} \\
    \midrule
    \endfirsthead
    \toprule
    Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC/ha)} & \thead{MAE$_{\text{test}}$ \\ (tC/ha)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC/ha)} \\
    \midrule
    \endhead
    \bottomrule
    \endlastfoot
        \textbf{Stack5 GradientBoosting} & \textbf{0.724} & \textbf{25.889} & \textbf{14.531} & \textbf{35.280} & \textbf{47.157} & \textbf{-0.579} \\
        Stack1 MLP$_{\text{global}}$ & 0.722 & 26.683 & 14.582 & 35.106 & 46.515 & 0.117 \\
        Stack2 MLP$_{\text{global}}$ & 0.721 & 26.726 & 14.475 & 34.848 & 46.320 & -0.131 \\
        Stack5 MLP$_{\text{global}}$ & 0.721 & 26.742 & 14.502 & 34.913 & 46.347 & 0.077 \\
        Stack3 GradientBoosting & 0.720 & 26.054 & 14.847 & 36.045 & 48.023 & -0.747 \\
        Stack1 LinearRegression$_{\text{global}}$ & 0.719 & 26.827 & 14.609 & 35.173 & 46.712 & -0.140 \\
        Stack1 Ridge$_{\text{global}}$ & 0.719 & 26.829 & 14.609 & 35.172 & 46.709 & -0.142 \\
        Stack5 MLP & 0.718 & 26.127 & 14.452 & 35.088 & 46.430 & -0.607 \\
        Stack4 MLP$_{\text{global}}$ & 0.718 & 26.852 & 14.479 & 34.859 & 46.260 & -0.328 \\
        Stack2 LinearRegression$_{\text{global}}$ & 0.718 & 26.877 & 14.505 & 34.922 & 46.440 & -0.112 \\
        Stack2 Ridge$_{\text{global}}$ & 0.718 & 26.883 & 14.507 & 34.927 & 46.440 & -0.114 \\
        Stack4 MLP & 0.717 & 26.177 & 14.550 & 35.326 & 46.579 & -0.495 \\
        Stack5 LinearRegression$_{\text{global}}$ & 0.717 & 26.905 & 14.552 & 35.034 & 46.852 & -0.053 \\
        Stack5 Ridge$_{\text{global}}$ & 0.717 & 26.910 & 14.547 & 35.022 & 46.818 & -0.064 \\
        Stack4 LinearRegression$_{\text{global}}$ & 0.717 & 26.912 & 14.590 & 35.127 & 46.948 & -0.056 \\
        Stack4 Ridge$_{\text{global}}$ & 0.717 & 26.913 & 14.584 & 35.112 & 46.912 & -0.066 \\
        Stack5 LinearRegression & 0.716 & 26.247 & 14.600 & 35.445 & 47.345 & -0.545 \\
        Stack5 Ridge & 0.716 & 26.247 & 14.599 & 35.445 & 47.344 & -0.545 \\
        Stack1 MLP & 0.716 & 26.257 & 14.669 & 35.614 & 47.371 & -0.555 \\
        Stack4 LinearRegression & 0.715 & 26.271 & 14.621 & 35.498 & 47.385 & -0.553 \\
        Stack4 Ridge & 0.715 & 26.271 & 14.621 & 35.498 & 47.385 & -0.553 \\
        Stack2 GradientBoosting$_{\text{global}}$ & 0.715 & 27.001 & 14.536 & 34.996 & 46.544 & -0.181 \\
        Stack1 Ridge & 0.714 & 26.339 & 14.689 & 35.663 & 47.140 & -0.500 \\
        Stack1 LinearRegression & 0.714 & 26.339 & 14.689 & 35.663 & 47.140 & -0.500 \\
        Stack4 GradientBoosting & 0.713 & 26.383 & 14.607 & 35.464 & 47.377 & -0.674 \\
        Stack2 MLP & 0.712 & 26.401 & 14.587 & 35.414 & 46.924 & -0.528 \\
        Stack3 LinearRegression$_{\text{global}}$ & 0.712 & 27.155 & 14.647 & 35.265 & 47.109 & -0.126 \\
        Stack3 Ridge$_{\text{global}}$ & 0.712 & 27.157 & 14.640 & 35.248 & 47.066 & -0.138 \\
        Stack2 LinearRegression & 0.712 & 26.429 & 14.632 & 35.523 & 47.316 & -0.535 \\
        Stack2 Ridge & 0.712 & 26.429 & 14.632 & 35.523 & 47.316 & -0.535 \\
        Stack3 MLP$_{\text{global}}$ & 0.711 & 27.212 & 14.529 & 34.979 & 46.365 & -0.705 \\
        Stack2 SVR$_{\text{global}}$ & 0.711 & 27.212 & 14.225 & 34.247 & 45.732 & -2.514 \\
        Stack4 SVR$_{\text{global}}$ & 0.711 & 27.225 & 14.249 & 34.305 & 46.002 & -2.559 \\
        Stack5 SVR$_{\text{global}}$ & 0.710 & 27.237 & 14.218 & 34.232 & 45.909 & -2.549 \\
        Stack1 SVR$_{\text{global}}$ & 0.710 & 27.240 & 14.307 & 34.445 & 45.502 & -2.747 \\
        Stack3 MLP & 0.709 & 26.544 & 14.875 & 36.114 & 47.914 & -0.401 \\
        Stack5 SVR & 0.709 & 26.550 & 14.241 & 34.575 & 47.201 & -4.012 \\
        Stack5 RandomForest & 0.708 & 26.594 & 15.177 & 36.847 & 48.063 & 0.095 \\
        Stack4 SVR & 0.708 & 26.599 & 14.320 & 34.766 & 47.387 & -4.112 \\
        Stack3 SVR$_{\text{global}}$ & 0.706 & 27.453 & 14.305 & 34.441 & 46.147 & -2.626 \\
        Stack1 SVR & 0.705 & 26.729 & 14.400 & 34.961 & 46.958 & -3.988 \\
        Stack2 SVR & 0.704 & 26.784 & 14.279 & 34.667 & 47.082 & -4.027 \\
        Stack3 LinearRegression & 0.703 & 26.832 & 14.942 & 36.276 & 48.431 & -0.664 \\
        Stack3 Ridge & 0.703 & 26.832 & 14.942 & 36.276 & 48.431 & -0.664 \\
        Stack4 RandomForest & 0.702 & 26.897 & 15.442 & 37.490 & 48.861 & -0.050 \\
        Stack5 GradientBoosting$_{\text{global}}$ & 0.701 & 27.690 & 14.571 & 35.081 & 46.490 & -0.240 \\
        Stack1 GradientBoosting$_{\text{global}}$ & 0.699 & 27.740 & 14.641 & 35.250 & 46.608 & -0.367 \\
        Stack3 GradientBoosting$_{\text{global}}$ & 0.699 & 27.770 & 14.642 & 35.253 & 46.471 & -0.282 \\
        Stack3 SVR & 0.696 & 27.146 & 14.613 & 35.477 & 48.257 & -4.409 \\
        Stack1 GradientBoosting & 0.695 & 27.176 & 14.815 & 35.967 & 47.547 & -0.476 \\
        Stack4 GradientBoosting$_{\text{global}}$ & 0.693 & 28.026 & 14.640 & 35.247 & 46.441 & -0.166 \\
        Stack2 RandomForest$_{\text{global}}$ & 0.690 & 28.187 & 15.317 & 36.876 & 47.788 & 0.139 \\
        Stack2 GradientBoosting & 0.690 & 27.429 & 14.710 & 35.714 & 47.413 & -0.599 \\
        Stack2 RandomForest & 0.688 & 27.488 & 15.562 & 37.782 & 48.807 & -0.134 \\
        Stack3 RandomForest & 0.685 & 27.622 & 15.914 & 38.636 & 49.763 & -0.395 \\
        Stack1 RandomForest$_{\text{global}}$ & 0.681 & 28.578 & 15.738 & 37.891 & 48.764 & -0.085 \\
        Stack5 RandomForest$_{\text{global}}$ & 0.679 & 28.674 & 15.225 & 36.656 & 47.660 & 0.534 \\
        Stack4 RandomForest$_{\text{global}}$ & 0.673 & 28.956 & 15.339 & 36.929 & 47.706 & 0.394 \\
        Stack1 RandomForest & 0.672 & 28.193 & 16.193 & 39.314 & 50.293 & -0.396 \\
        Stack3 RandomForest$_{\text{global}}$ & 0.667 & 29.218 & 15.575 & 37.499 & 48.075 & 0.408 \\
    \end{longtable}
    }

\subsubsection{Toneladas de carbono}

\subsubsubsection{Modelos base}

En la Tabla \ref{tab:base_comp_ifn2_carbono_bruto4} se muestra el resumen del rendimiento de los modelos para la predicción de la variable de carbono en toneladas (carbono\_bruto4) con el conjunto de datos que emplea IFN2 como explicativo.

\begin{table}[H]
    \centering
    \caption{Comparativa Modelos Base en IFN2 (carbono\_bruto4)}
    \label{tab:base_comp_ifn2_carbono_bruto4}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lrrrrrr}
        \toprule
        Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC)} & \thead{MAE$_{\text{test}}$ \\ (tC)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC)} \\
        \midrule
        \textbf{CatBoost$_{\text{global}}$} & \textbf{0.765} & \textbf{18.533} & \textbf{10.402} & \textbf{35.470} & \textbf{69.561} & \textbf{-0.342} \\
        LightGBM$_{\text{global}}$ & 0.762 & 18.627 & 10.417 & 35.522 & 69.531 & -0.313 \\
        XGBoost$_{\text{global}}$ & 0.762 & 18.644 & 10.428 & 35.559 & 69.353 & -0.384 \\
        GBDT$_{\text{global}}$ & 0.759 & 18.765 & 10.474 & 35.716 & 69.410 & -0.407 \\
        CatBoost & 0.748 & 19.071 & 11.036 & 38.035 & 72.181 & 0.034 \\
        MLP$_{\text{global}}$ & 0.747 & 19.220 & 10.732 & 36.597 & 71.377 & -1.309 \\
        LightGBM & 0.747 & 19.121 & 10.858 & 37.423 & 71.698 & 0.042 \\
        XGBoost & 0.742 & 19.300 & 10.962 & 37.780 & 71.609 & -0.004 \\
        BaggedDT$_{\text{global}}$ & 0.738 & 19.550 & 11.216 & 38.246 & 71.630 & -0.749 \\
        RandomForest$_{\text{global}}$ & 0.734 & 19.694 & 10.928 & 37.264 & 68.279 & -1.503 \\
        GBDT & 0.732 & 19.692 & 11.550 & 39.809 & 73.147 & 0.016 \\
        RandomForest & 0.729 & 19.770 & 11.294 & 38.925 & 70.570 & 0.013 \\
        BaggedDT & 0.728 & 19.832 & 11.349 & 39.114 & 70.434 & 0.182 \\
        BayesianNN & 0.693 & 21.069 & 12.830 & 44.220 & 79.896 & -0.076 \\
        MLP & 0.690 & 21.162 & 11.419 & 39.356 & 76.074 & -3.511 \\
        BayesianNN$_{\text{global}}$ & 0.683 & 21.530 & 12.827 & 43.741 & 78.379 & -0.859 \\
        KNN & 0.632 & 23.046 & 12.994 & 44.784 & 76.109 & -3.677 \\
        SVR$_{\text{global}}$ & 0.577 & 24.861 & 12.305 & 41.961 & 75.324 & -7.293 \\
        SVR & 0.566 & 25.034 & 12.818 & 44.177 & 77.708 & -6.476 \\
        AdaBoost & -0.538 & 47.138 & 41.856 & 144.261 & 114.694 & 37.323 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsubsubsection{Modelos con stacking}

En la Tabla \ref{tab:stack_comp_ifn2_carbono_bruto4} se muestra el resumen del rendimiento de los modelos de stacking para la predicción de la variable de carbono en toneladas (carbono\_bruto4) con el conjunto de datos que emplea IFN2 como explicativo.

{
{
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \begin{longtable}{p{4.5cm}rrrrrr}
    \caption{Comparativa Modelos Stacking en IFN2 (carbono\_bruto4)}
    \label{tab:stack_comp_ifn2_carbono_bruto4} \\
    \toprule
    Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC)} & \thead{MAE$_{\text{test}}$ \\ (tC)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC)} \\
    \midrule
    \endfirsthead
    \toprule
    Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC)} & \thead{MAE$_{\text{test}}$ \\ (tC)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC)} \\
    \midrule
    \endhead
    \bottomrule
    \endlastfoot
        \textbf{Stack5 MLP$_{\text{global}}$} & \textbf{0.768} & \textbf{18.405} & \textbf{10.210} & \textbf{34.817} & \textbf{68.129} & \textbf{-0.195} \\
        Stack5 LinearRegression$_{\text{global}}$ & 0.768 & 18.418 & 10.306 & 35.144 & 69.309 & -0.169 \\
        Stack5 Ridge$_{\text{global}}$ & 0.768 & 18.418 & 10.306 & 35.143 & 69.320 & -0.170 \\
        Stack4 MLP$_{\text{global}}$ & 0.767 & 18.433 & 10.229 & 34.881 & 68.095 & 0.030 \\
        Stack4 LinearRegression$_{\text{global}}$ & 0.767 & 18.435 & 10.297 & 35.112 & 69.291 & -0.232 \\
        Stack4 Ridge$_{\text{global}}$ & 0.767 & 18.435 & 10.296 & 35.111 & 69.288 & -0.233 \\
        Stack3 MLP$_{\text{global}}$ & 0.767 & 18.450 & 10.184 & 34.726 & 67.782 & -0.295 \\
        Stack5 GradientBoosting$_{\text{global}}$ & 0.767 & 18.457 & 10.224 & 34.863 & 68.228 & -0.393 \\
        Stack3 Ridge$_{\text{global}}$ & 0.767 & 18.466 & 10.318 & 35.185 & 69.212 & -0.277 \\
        Stack3 LinearRegression$_{\text{global}}$ & 0.767 & 18.466 & 10.318 & 35.185 & 69.212 & -0.277 \\
        Stack4 GradientBoosting$_{\text{global}}$ & 0.766 & 18.485 & 10.216 & 34.837 & 68.125 & -0.407 \\
        Stack2 LinearRegression$_{\text{global}}$ & 0.765 & 18.521 & 10.333 & 35.236 & 69.359 & -0.252 \\
        Stack2 Ridge$_{\text{global}}$ & 0.765 & 18.521 & 10.333 & 35.236 & 69.360 & -0.253 \\
        Stack3 GradientBoosting$_{\text{global}}$ & 0.765 & 18.529 & 10.230 & 34.886 & 68.141 & -0.463 \\
        Stack2 MLP$_{\text{global}}$ & 0.765 & 18.531 & 10.285 & 35.071 & 68.797 & -0.207 \\
        Stack5 SVR$_{\text{global}}$ & 0.763 & 18.595 & 10.155 & 34.628 & 68.878 & -1.814 \\
        Stack4 SVR$_{\text{global}}$ & 0.763 & 18.601 & 10.169 & 34.676 & 68.822 & -1.826 \\
        Stack1 MLP$_{\text{global}}$ & 0.763 & 18.602 & 10.261 & 34.989 & 67.896 & -0.730 \\
        Stack1 Ridge$_{\text{global}}$ & 0.763 & 18.606 & 10.377 & 35.385 & 69.330 & -0.345 \\
        Stack1 LinearRegression$_{\text{global}}$ & 0.763 & 18.606 & 10.377 & 35.385 & 69.330 & -0.345 \\
        Stack2 GradientBoosting$_{\text{global}}$ & 0.763 & 18.616 & 10.297 & 35.114 & 68.617 & -0.412 \\
        Stack3 SVR$_{\text{global}}$ & 0.762 & 18.643 & 10.193 & 34.758 & 68.689 & -1.883 \\
        Stack1 GradientBoosting$_{\text{global}}$ & 0.761 & 18.676 & 10.316 & 35.177 & 68.372 & -0.484 \\
        Stack2 SVR$_{\text{global}}$ & 0.761 & 18.678 & 10.222 & 34.856 & 68.947 & -1.763 \\
        Stack1 SVR$_{\text{global}}$ & 0.758 & 18.821 & 10.260 & 34.986 & 68.943 & -2.023 \\
        Stack5 RandomForest$_{\text{global}}$ & 0.756 & 18.879 & 10.606 & 36.167 & 66.996 & 0.138 \\
        Stack4 RandomForest$_{\text{global}}$ & 0.756 & 18.886 & 10.704 & 36.501 & 67.217 & 0.084 \\
        Stack4 MLP & 0.755 & 18.828 & 10.636 & 36.658 & 70.675 & 0.274 \\
        Stack4 Ridge & 0.754 & 18.860 & 10.734 & 36.997 & 71.361 & 0.044 \\
        Stack4 LinearRegression & 0.754 & 18.860 & 10.734 & 36.997 & 71.364 & 0.044 \\
        Stack5 GradientBoosting & 0.753 & 18.880 & 10.692 & 36.852 & 70.603 & -0.022 \\
        Stack5 MLP & 0.753 & 18.890 & 10.632 & 36.643 & 71.389 & 0.045 \\
        Stack5 Ridge & 0.753 & 18.893 & 10.746 & 37.038 & 71.326 & 0.016 \\
        Stack3 MLP & 0.753 & 18.896 & 10.746 & 37.036 & 71.008 & 0.081 \\
        Stack4 GradientBoosting & 0.752 & 18.921 & 10.654 & 36.720 & 70.494 & 0.029 \\
        Stack2 MLP & 0.751 & 18.954 & 10.765 & 37.102 & 70.755 & -0.001 \\
        Stack3 Ridge & 0.751 & 18.977 & 10.873 & 37.473 & 71.697 & 0.076 \\
        Stack3 LinearRegression & 0.751 & 18.978 & 10.873 & 37.474 & 71.695 & 0.076 \\
        Stack2 GradientBoosting & 0.751 & 18.978 & 10.769 & 37.116 & 70.909 & 0.002 \\
        Stack4 SVR & 0.750 & 18.988 & 10.523 & 36.267 & 72.240 & -2.179 \\
        Stack2 Ridge & 0.750 & 18.989 & 10.793 & 37.200 & 71.359 & -0.048 \\
        Stack2 LinearRegression & 0.750 & 18.989 & 10.793 & 37.200 & 71.357 & -0.048 \\
        Stack5 SVR & 0.750 & 18.993 & 10.530 & 36.293 & 72.199 & -2.129 \\
        Stack3 GradientBoosting & 0.749 & 19.027 & 10.783 & 37.164 & 70.757 & 0.076 \\
        Stack1 MLP & 0.749 & 19.043 & 10.736 & 37.001 & 70.468 & -0.031 \\
        Stack1 Ridge & 0.749 & 19.048 & 10.777 & 37.145 & 71.433 & -0.008 \\
        Stack1 LinearRegression & 0.749 & 19.048 & 10.777 & 37.145 & 71.434 & -0.008 \\
        Stack3 SVR & 0.747 & 19.116 & 10.656 & 36.726 & 72.643 & -2.243 \\
        Stack2 SVR & 0.747 & 19.116 & 10.602 & 36.542 & 72.634 & -2.230 \\
        Stack1 GradientBoosting & 0.746 & 19.174 & 10.778 & 37.146 & 70.768 & -0.046 \\
        Stack3 RandomForest$_{\text{global}}$ & 0.745 & 19.286 & 10.840 & 36.965 & 67.152 & -0.105 \\
        Stack1 SVR & 0.745 & 19.187 & 10.596 & 36.521 & 72.624 & -2.197 \\
        Stack2 RandomForest$_{\text{global}}$ & 0.741 & 19.455 & 10.979 & 37.440 & 69.244 & -0.110 \\
        Stack5 RandomForest & 0.734 & 19.588 & 11.135 & 38.377 & 68.643 & 0.589 \\
        Stack1 RandomForest$_{\text{global}}$ & 0.734 & 19.711 & 11.235 & 38.313 & 68.420 & -0.278 \\
        Stack3 RandomForest & 0.727 & 19.848 & 11.483 & 39.578 & 69.750 & 0.543 \\
        Stack4 RandomForest & 0.727 & 19.848 & 11.194 & 38.580 & 68.556 & 0.550 \\
        Stack2 RandomForest & 0.720 & 20.098 & 11.538 & 39.768 & 71.966 & 0.196 \\
        Stack1 RandomForest & 0.713 & 20.375 & 11.767 & 40.555 & 70.731 & 0.068 \\
    \end{longtable}
    }

\subsection{Resultados IFN3}

\subsubsection{Toneladas de carbono por hectárea}

\subsubsubsection{Modelos base}

Una vez entrenados los modelos, algunos parámetros globales como el $R^2$, RMSE y MAE se presentan en la Tabla \ref{tab:base_comp_ifn3_c4}.

\begin{table}[H]
    \centering
    \caption{Comparativa Modelos Base en IFN3 (c4)}
    \label{tab:base_comp_ifn3_c4}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lrrrrrr}
        \toprule
        Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC/ha)} & \thead{MAE$_{\text{test}}$ \\ (tC/ha)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC/ha)} \\
        \midrule
        \textbf{LightGBM} & \textbf{0.858} & \textbf{17.816} & \textbf{9.246} & \textbf{24.609} & \textbf{37.327} & \textbf{0.017} \\
        XGBoost & 0.851 & 18.232 & 9.203 & 24.494 & 37.033 & -0.021 \\
        CatBoost & 0.849 & 18.385 & 9.746 & 25.941 & 40.139 & -0.114 \\
        GBDT & 0.843 & 18.718 & 9.893 & 26.330 & 40.046 & 0.005 \\
        MLP & 0.840 & 18.947 & 9.296 & 24.743 & 36.827 & -3.555 \\
        GBDT$_{\text{global}}$ & 0.839 & 19.408 & 9.559 & 25.792 & 38.647 & 0.378 \\
        XGBoost$_{\text{global}}$ & 0.839 & 19.424 & 9.548 & 25.763 & 39.032 & 0.206 \\
        CatBoost$_{\text{global}}$ & 0.838 & 19.500 & 9.433 & 25.451 & 39.540 & 0.056 \\
        LightGBM$_{\text{global}}$ & 0.836 & 19.580 & 9.602 & 25.909 & 39.178 & 0.237 \\
        MLP$_{\text{global}}$ & 0.833 & 19.791 & 10.008 & 27.005 & 44.150 & 0.107 \\
        BaggedDT & 0.820 & 20.085 & 10.699 & 28.476 & 40.585 & 0.098 \\
        BaggedDT$_{\text{global}}$ & 0.800 & 21.649 & 11.058 & 29.838 & 44.515 & 1.538 \\
        RandomForest & 0.796 & 21.353 & 10.839 & 28.849 & 40.827 & -0.092 \\
        RandomForest$_{\text{global}}$ & 0.789 & 22.223 & 11.109 & 29.974 & 41.901 & 1.399 \\
        BayesianNN & 0.774 & 22.507 & 12.332 & 32.823 & 55.074 & 0.201 \\
        KNN & 0.756 & 23.357 & 12.585 & 33.495 & 48.029 & -5.178 \\
        BayesianNN$_{\text{global}}$ & 0.743 & 24.561 & 12.643 & 34.113 & 56.226 & 0.572 \\
        SVR & 0.737 & 24.279 & 9.877 & 26.289 & 37.234 & -3.632 \\
        SVR$_{\text{global}}$ & 0.613 & 30.113 & 11.708 & 31.590 & 42.962 & -4.778 \\
        AdaBoost & 0.314 & 39.189 & 33.180 & 88.310 & 91.289 & 22.855 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsubsubsection{Modelos con stacking}

En la Tabla \ref{tab:stack_comp_ifn3_c4} se muestra el resumen del rendimiento de los modelos de stacking para la predicción de la variable de carbono en tC/ha con el conjunto de datos que emplea IFN3 como explicativo.

{
{
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \begin{longtable}{p{4.5cm}rrrrrr}
    \caption{Comparativa Modelos Stacking en IFN3 (c4)}
    \label{tab:stack_comp_ifn3_c4} \\
    \toprule
    Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC/ha)} & \thead{MAE$_{\text{test}}$ \\ (tC/ha)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC/ha)} \\
    \midrule
    \endfirsthead
    \toprule
    Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC/ha)} & \thead{MAE$_{\text{test}}$ \\ (tC/ha)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC/ha)} \\
    \midrule
    \endhead
    \bottomrule
    \endlastfoot
        \textbf{Stack4 GradientBoosting} & \textbf{0.866} & \textbf{17.303} & \textbf{9.134} & \textbf{24.310} & \textbf{37.215} & \textbf{-0.009} \\
        Stack5 GradientBoosting & 0.865 & 17.369 & 9.037 & 24.052 & 36.846 & -0.007 \\
        Stack2 GradientBoosting & 0.863 & 17.537 & 9.097 & 24.211 & 37.106 & 0.005 \\
        Stack5 MLP & 0.862 & 17.572 & 8.950 & 23.822 & 36.332 & -0.335 \\
        Stack5 LinearRegression & 0.861 & 17.644 & 9.049 & 24.083 & 37.004 & -0.013 \\
        Stack5 Ridge & 0.861 & 17.646 & 9.050 & 24.087 & 37.003 & -0.012 \\
        Stack4 MLP & 0.861 & 17.647 & 9.147 & 24.346 & 36.813 & 0.171 \\
        Stack2 MLP & 0.860 & 17.704 & 9.038 & 24.054 & 36.404 & -0.294 \\
        Stack4 LinearRegression & 0.860 & 17.718 & 9.190 & 24.459 & 37.349 & 0.011 \\
        Stack4 Ridge & 0.860 & 17.720 & 9.191 & 24.463 & 37.351 & 0.011 \\
        Stack1 GradientBoosting & 0.859 & 17.740 & 9.273 & 24.681 & 37.369 & 0.029 \\
        Stack2 LinearRegression & 0.859 & 17.767 & 9.097 & 24.213 & 36.750 & -0.004 \\
        Stack2 Ridge & 0.859 & 17.773 & 9.107 & 24.240 & 36.785 & -0.004 \\
        Stack5 SVR & 0.858 & 17.816 & 8.879 & 23.631 & 37.125 & -1.871 \\
        Stack5 RandomForest & 0.858 & 17.819 & 9.445 & 25.139 & 37.517 & 0.362 \\
        Stack1 MLP & 0.858 & 17.834 & 9.264 & 24.657 & 37.281 & 0.119 \\
        Stack4 SVR & 0.858 & 17.851 & 9.011 & 23.984 & 37.554 & -1.902 \\
        Stack1 LinearRegression & 0.857 & 17.873 & 9.244 & 24.604 & 36.837 & 0.023 \\
        Stack1 Ridge & 0.857 & 17.875 & 9.245 & 24.606 & 36.831 & 0.023 \\
        Stack2 SVR & 0.856 & 17.954 & 8.935 & 23.781 & 37.175 & -1.851 \\
        Stack3 GradientBoosting & 0.855 & 18.026 & 9.534 & 25.375 & 38.431 & 0.002 \\
        Stack1 SVR & 0.854 & 18.053 & 9.114 & 24.257 & 37.328 & -1.845 \\
        Stack3 MLP & 0.852 & 18.171 & 9.500 & 25.285 & 37.589 & 0.001 \\
        Stack4 RandomForest & 0.852 & 18.199 & 9.748 & 25.946 & 38.468 & 0.330 \\
        Stack3 LinearRegression & 0.850 & 18.309 & 9.580 & 25.498 & 38.816 & 0.007 \\
        Stack3 Ridge & 0.850 & 18.316 & 9.581 & 25.500 & 38.784 & 0.008 \\
        Stack3 RandomForest & 0.849 & 18.375 & 10.159 & 27.039 & 39.787 & 0.336 \\
        Stack5 MLP$_{\text{global}}$ & 0.848 & 18.892 & 9.178 & 24.763 & 36.606 & 0.347 \\
        Stack2 MLP$_{\text{global}}$ & 0.847 & 18.943 & 9.332 & 25.180 & 37.530 & 0.382 \\
        Stack3 SVR & 0.846 & 18.552 & 9.397 & 25.010 & 38.601 & -2.106 \\
        Stack4 MLP$_{\text{global}}$ & 0.845 & 19.067 & 9.180 & 24.770 & 36.741 & 0.146 \\
        Stack5 Ridge$_{\text{global}}$ & 0.844 & 19.100 & 9.252 & 24.965 & 38.537 & 0.140 \\
        Stack5 LinearRegression$_{\text{global}}$ & 0.844 & 19.102 & 9.253 & 24.966 & 38.597 & 0.131 \\
        Stack2 Ridge$_{\text{global}}$ & 0.844 & 19.115 & 9.355 & 25.241 & 38.497 & 0.193 \\
        Stack2 LinearRegression$_{\text{global}}$ & 0.844 & 19.117 & 9.356 & 25.244 & 38.525 & 0.191 \\
        Stack4 Ridge$_{\text{global}}$ & 0.843 & 19.154 & 9.278 & 25.033 & 38.451 & 0.184 \\
        Stack4 LinearRegression$_{\text{global}}$ & 0.843 & 19.156 & 9.277 & 25.030 & 38.505 & 0.173 \\
        Stack3 Ridge$_{\text{global}}$ & 0.843 & 19.187 & 9.305 & 25.107 & 38.570 & 0.218 \\
        Stack3 LinearRegression$_{\text{global}}$ & 0.843 & 19.188 & 9.304 & 25.104 & 38.641 & 0.205 \\
        Stack3 MLP$_{\text{global}}$ & 0.842 & 19.216 & 9.188 & 24.790 & 36.845 & -0.169 \\
        Stack5 SVR$_{\text{global}}$ & 0.840 & 19.367 & 9.101 & 24.557 & 40.002 & -2.063 \\
        Stack2 SVR$_{\text{global}}$ & 0.840 & 19.369 & 9.185 & 24.783 & 39.985 & -2.057 \\
        Stack4 SVR$_{\text{global}}$ & 0.839 & 19.400 & 9.116 & 24.597 & 39.978 & -2.045 \\
        Stack3 SVR$_{\text{global}}$ & 0.839 & 19.423 & 9.137 & 24.655 & 40.013 & -2.018 \\
        Stack2 RandomForest & 0.839 & 18.988 & 9.887 & 26.314 & 38.705 & 0.293 \\
        Stack1 MLP$_{\text{global}}$ & 0.839 & 19.436 & 9.564 & 25.804 & 37.443 & 0.770 \\
        Stack3 GradientBoosting$_{\text{global}}$ & 0.837 & 19.547 & 9.290 & 25.067 & 37.493 & 0.310 \\
        Stack1 LinearRegression$_{\text{global}}$ & 0.837 & 19.564 & 9.582 & 25.854 & 38.721 & 0.337 \\
        Stack1 Ridge$_{\text{global}}$ & 0.837 & 19.565 & 9.582 & 25.854 & 38.714 & 0.339 \\
        Stack1 RandomForest & 0.835 & 19.224 & 10.450 & 27.814 & 40.024 & 0.233 \\
        Stack5 GradientBoosting$_{\text{global}}$ & 0.833 & 19.768 & 9.291 & 25.069 & 37.539 & 0.270 \\
        Stack1 GradientBoosting$_{\text{global}}$ & 0.832 & 19.820 & 9.600 & 25.903 & 38.177 & 0.508 \\
        Stack1 SVR$_{\text{global}}$ & 0.832 & 19.852 & 9.388 & 25.330 & 39.687 & -1.927 \\
        Stack4 GradientBoosting$_{\text{global}}$ & 0.830 & 19.940 & 9.344 & 25.213 & 37.491 & 0.304 \\
        Stack2 GradientBoosting$_{\text{global}}$ & 0.826 & 20.206 & 9.499 & 25.631 & 38.253 & 0.331 \\
        Stack5 RandomForest$_{\text{global}}$ & 0.820 & 20.540 & 9.871 & 26.635 & 38.299 & 0.692 \\
        Stack3 RandomForest$_{\text{global}}$ & 0.807 & 21.266 & 10.271 & 27.713 & 39.177 & 0.574 \\
        Stack4 RandomForest$_{\text{global}}$ & 0.804 & 21.415 & 10.123 & 27.314 & 38.710 & 0.763 \\
        Stack2 RandomForest$_{\text{global}}$ & 0.802 & 21.562 & 10.342 & 27.905 & 39.864 & 0.540 \\
        Stack1 RandomForest$_{\text{global}}$ & 0.792 & 22.090 & 10.962 & 29.576 & 40.925 & 0.731 \\
    \end{longtable}
    }

\subsubsection{Toneladas de carbono}

\subsubsubsection{Modelos base}

En la Tabla \ref{tab:base_comp_ifn3_carbono_bruto4} se muestra el resumen del rendimiento de los modelos para la predicción de la variable de carbono en toneladas (carbono\_bruto4) con el conjunto de datos que emplea IFN3 como explicativo.

\begin{table}[H]
    \centering
    \caption{Comparativa Modelos Base en IFN3 (carbono\_bruto4)}
    \label{tab:base_comp_ifn3_carbono_bruto4}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lrrrrrr}
        \toprule
        Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC)} & \thead{MAE$_{\text{test}}$ \\ (tC)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC)} \\
        \midrule
        \textbf{LightGBM} & \textbf{0.898} & \textbf{10.641} & \textbf{4.687} & \textbf{27.801} & \textbf{110.183} & \textbf{-0.007} \\
        CatBoost & 0.896 & 10.737 & 4.789 & 28.405 & 110.952 & -0.059 \\
        XGBoost & 0.895 & 10.761 & 4.713 & 27.957 & 109.669 & -0.008 \\
        CatBoost$_{\text{global}}$ & 0.889 & 10.931 & 4.812 & 28.723 & 111.600 & -0.059 \\
        GBDT & 0.888 & 11.134 & 5.044 & 29.920 & 111.422 & -0.042 \\
        MLP & 0.885 & 11.287 & 4.803 & 28.488 & 111.603 & -0.544 \\
        LightGBM$_{\text{global}}$ & 0.884 & 11.153 & 4.863 & 29.023 & 111.679 & -0.073 \\
        XGBoost$_{\text{global}}$ & 0.883 & 11.230 & 4.859 & 28.999 & 111.190 & -0.036 \\
        GBDT$_{\text{global}}$ & 0.881 & 11.327 & 4.937 & 29.463 & 111.656 & -0.002 \\
        MLP$_{\text{global}}$ & 0.879 & 11.431 & 5.122 & 30.573 & 114.182 & -0.326 \\
        BaggedDT & 0.873 & 11.830 & 5.183 & 30.745 & 99.986 & -0.135 \\
        RandomForest & 0.870 & 11.965 & 5.217 & 30.944 & 109.139 & -0.239 \\
        BaggedDT$_{\text{global}}$ & 0.866 & 11.997 & 5.410 & 32.287 & 110.035 & 0.589 \\
        RandomForest$_{\text{global}}$ & 0.865 & 12.053 & 5.330 & 31.814 & 105.593 & 0.434 \\
        BayesianNN & 0.836 & 13.455 & 6.773 & 40.173 & 120.931 & 0.010 \\
        BayesianNN$_{\text{global}}$ & 0.824 & 13.774 & 7.040 & 42.020 & 121.650 & 0.265 \\
        SVR & 0.810 & 14.504 & 5.269 & 31.256 & 112.607 & -2.176 \\
        KNN & 0.793 & 15.128 & 6.251 & 37.078 & 102.871 & -2.897 \\
        SVR$_{\text{global}}$ & 0.730 & 17.033 & 6.154 & 36.728 & 114.785 & -2.987 \\
        AdaBoost & 0.349 & 26.829 & 23.857 & 141.512 & 139.341 & 20.684 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsubsubsection{Modelos con stacking}

En la Tabla \ref{tab:stack_comp_ifn3_carbono_bruto4} se muestra el resumen del rendimiento de los modelos con stacking para la predicción de la variable de carbono en toneladas (carbono\_bruto4) con el conjunto de datos que emplea IFN3 como explicativo.

{
{
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \begin{longtable}{p{4.5cm}rrrrrr}
    \caption{Comparativa Modelos Stacking en IFN3 (carbono\_bruto4)}
    \label{tab:stack_comp_ifn3_carbono_bruto4} \\
    \toprule
    Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC)} & \thead{MAE$_{\text{test}}$ \\ (tC)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC)} \\
    \midrule
    \endfirsthead
    \toprule
    Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC)} & \thead{MAE$_{\text{test}}$ \\ (tC)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC)} \\
    \midrule
    \endhead
    \bottomrule
    \endlastfoot
        \textbf{Stack4 MLP} & \textbf{0.902} & \textbf{10.407} & \textbf{4.517} & \textbf{26.795} & \textbf{107.868} & \textbf{0.049} \\
        Stack5 GradientBoosting & 0.901 & 10.433 & 4.540 & 26.930 & 109.331 & 0.030 \\
        Stack5 MLP & 0.901 & 10.447 & 4.454 & 26.417 & 107.446 & -0.368 \\
        Stack4 GradientBoosting & 0.901 & 10.450 & 4.541 & 26.936 & 109.286 & 0.037 \\
        Stack5 LinearRegression & 0.900 & 10.492 & 4.624 & 27.426 & 110.839 & 0.025 \\
        Stack5 Ridge & 0.900 & 10.494 & 4.624 & 27.428 & 110.757 & 0.022 \\
        Stack4 LinearRegression & 0.900 & 10.511 & 4.648 & 27.570 & 110.932 & 0.020 \\
        Stack4 Ridge & 0.900 & 10.511 & 4.648 & 27.570 & 110.941 & 0.020 \\
        Stack2 MLP & 0.899 & 10.540 & 4.581 & 27.173 & 108.573 & -0.035 \\
        Stack2 LinearRegression & 0.899 & 10.572 & 4.651 & 27.590 & 110.865 & 0.011 \\
        Stack3 MLP & 0.899 & 10.572 & 4.611 & 27.348 & 106.609 & 0.170 \\
        Stack2 Ridge & 0.899 & 10.573 & 4.654 & 27.604 & 110.866 & 0.010 \\
        Stack2 GradientBoosting & 0.899 & 10.582 & 4.610 & 27.343 & 109.772 & 0.004 \\
        Stack1 MLP & 0.898 & 10.617 & 4.579 & 27.162 & 107.666 & -0.163 \\
        Stack1 LinearRegression & 0.898 & 10.625 & 4.667 & 27.682 & 111.022 & 0.011 \\
        Stack1 Ridge & 0.898 & 10.625 & 4.667 & 27.682 & 111.022 & 0.011 \\
        Stack5 SVR & 0.898 & 10.638 & 4.541 & 26.938 & 110.810 & -0.758 \\
        Stack4 SVR & 0.897 & 10.655 & 4.556 & 27.027 & 111.084 & -0.788 \\
        Stack1 GradientBoosting & 0.897 & 10.660 & 4.620 & 27.406 & 109.313 & 0.021 \\
        Stack3 GradientBoosting & 0.897 & 10.671 & 4.637 & 27.506 & 109.803 & 0.028 \\
        Stack3 LinearRegression & 0.897 & 10.674 & 4.757 & 28.218 & 111.578 & 0.014 \\
        Stack3 Ridge & 0.897 & 10.675 & 4.757 & 28.217 & 111.563 & 0.013 \\
        Stack2 SVR & 0.896 & 10.706 & 4.580 & 27.167 & 110.761 & -0.744 \\
        Stack5 RandomForest & 0.895 & 10.755 & 4.721 & 28.006 & 96.178 & 0.266 \\
        Stack4 RandomForest & 0.895 & 10.782 & 4.775 & 28.322 & 98.323 & 0.216 \\
        Stack1 SVR & 0.895 & 10.787 & 4.613 & 27.364 & 110.982 & -0.740 \\
        Stack3 SVR & 0.893 & 10.865 & 4.672 & 27.712 & 111.457 & -0.870 \\
        Stack4 MLP$_{\text{global}}$ & 0.890 & 10.862 & 4.621 & 27.580 & 109.222 & 0.226 \\
        Stack5 MLP$_{\text{global}}$ & 0.890 & 10.866 & 4.586 & 27.369 & 107.939 & 0.041 \\
        Stack3 MLP$_{\text{global}}$ & 0.890 & 10.880 & 4.544 & 27.119 & 106.128 & -0.009 \\
        Stack4 LinearRegression$_{\text{global}}$ & 0.890 & 10.890 & 4.741 & 28.298 & 111.854 & -0.075 \\
        Stack4 Ridge$_{\text{global}}$ & 0.890 & 10.891 & 4.741 & 28.297 & 111.872 & -0.075 \\
        Stack5 GradientBoosting$_{\text{global}}$ & 0.890 & 10.905 & 4.612 & 27.525 & 109.827 & -0.006 \\
        Stack5 LinearRegression$_{\text{global}}$ & 0.889 & 10.908 & 4.747 & 28.330 & 112.364 & -0.113 \\
        Stack5 Ridge$_{\text{global}}$ & 0.889 & 10.909 & 4.747 & 28.330 & 112.377 & -0.113 \\
        Stack3 LinearRegression$_{\text{global}}$ & 0.889 & 10.921 & 4.766 & 28.443 & 112.073 & -0.047 \\
        Stack3 Ridge$_{\text{global}}$ & 0.889 & 10.921 & 4.766 & 28.443 & 112.073 & -0.047 \\
        Stack3 GradientBoosting$_{\text{global}}$ & 0.888 & 10.959 & 4.626 & 27.612 & 109.664 & 0.036 \\
        Stack4 GradientBoosting$_{\text{global}}$ & 0.888 & 10.978 & 4.625 & 27.605 & 109.591 & 0.007 \\
        Stack3 RandomForest & 0.887 & 11.157 & 4.967 & 29.463 & 98.766 & 0.152 \\
        Stack2 RandomForest & 0.887 & 11.165 & 4.916 & 29.157 & 98.358 & 0.133 \\
        Stack4 SVR$_{\text{global}}$ & 0.887 & 11.053 & 4.682 & 27.941 & 112.576 & -0.961 \\
        Stack5 SVR$_{\text{global}}$ & 0.886 & 11.056 & 4.687 & 27.976 & 112.536 & -0.956 \\
        Stack2 MLP$_{\text{global}}$ & 0.886 & 11.058 & 4.709 & 28.105 & 109.662 & 0.075 \\
        Stack2 LinearRegression$_{\text{global}}$ & 0.886 & 11.065 & 4.803 & 28.663 & 112.597 & -0.094 \\
        Stack2 Ridge$_{\text{global}}$ & 0.886 & 11.065 & 4.802 & 28.662 & 112.594 & -0.094 \\
        Stack3 SVR$_{\text{global}}$ & 0.886 & 11.088 & 4.701 & 28.059 & 112.546 & -0.948 \\
        Stack1 MLP$_{\text{global}}$ & 0.885 & 11.102 & 4.680 & 27.934 & 107.602 & -0.133 \\
        Stack2 GradientBoosting$_{\text{global}}$ & 0.885 & 11.108 & 4.729 & 28.224 & 110.752 & -0.005 \\
        Stack1 Ridge$_{\text{global}}$ & 0.885 & 11.111 & 4.833 & 28.842 & 112.219 & -0.040 \\
        Stack1 LinearRegression$_{\text{global}}$ & 0.885 & 11.111 & 4.833 & 28.842 & 112.219 & -0.040 \\
        Stack1 GradientBoosting$_{\text{global}}$ & 0.884 & 11.171 & 4.732 & 28.242 & 109.678 & 0.048 \\
        Stack2 SVR$_{\text{global}}$ & 0.883 & 11.220 & 4.745 & 28.321 & 112.576 & -0.971 \\
        Stack1 SVR$_{\text{global}}$ & 0.882 & 11.271 & 4.760 & 28.409 & 112.572 & -0.951 \\
        Stack1 RandomForest & 0.881 & 11.483 & 5.127 & 30.409 & 95.521 & 0.089 \\
        Stack5 RandomForest$_{\text{global}}$ & 0.880 & 11.347 & 4.848 & 28.932 & 96.971 & 0.259 \\
        Stack4 RandomForest$_{\text{global}}$ & 0.879 & 11.429 & 4.919 & 29.356 & 96.704 & 0.192 \\
        Stack3 RandomForest$_{\text{global}}$ & 0.875 & 11.586 & 5.023 & 29.976 & 95.997 & 0.204 \\
        Stack2 RandomForest$_{\text{global}}$ & 0.873 & 11.692 & 5.131 & 30.625 & 102.247 & 0.209 \\
        Stack1 RandomForest$_{\text{global}}$ & 0.858 & 12.368 & 5.383 & 32.129 & 94.456 & 0.154 \\
    \end{longtable}
    }

\subsection{Resultados IFN2 e IFN3}


\subsubsection{Toneladas de carbono por hectárea}

\subsubsubsection{Modelos base}

Una vez entrenados los modelos, algunos parámetros globales como el $R^2$, RMSE y MAE se presentan en la Tabla \ref{tab:base_global_c4}

\begin{table}[H]
    \centering
    \caption{Modelos Base Globales (c4)}
    \label{tab:base_global_c4}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lrrrrrr}
        \toprule
        Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC/ha)} & \thead{MAE$_{\text{test}}$ \\ (tC/ha)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC/ha)} \\
        \midrule
        \textbf{LightGBM$_{\text{global}}$} & \textbf{0.787} & \textbf{22.767} & \textbf{11.650} & \textbf{29.978} & \textbf{42.370} & \textbf{0.134} \\
        XGBoost$_{\text{global}}$ & 0.784 & 22.952 & 11.590 & 29.824 & 42.204 & 0.055 \\
        CatBoost$_{\text{global}}$ & 0.783 & 22.990 & 11.607 & 29.866 & 42.904 & -0.021 \\
        GBDT$_{\text{global}}$ & 0.783 & 23.014 & 11.658 & 29.998 & 41.987 & 0.169 \\
        MLP$_{\text{global}}$ & 0.771 & 23.606 & 12.287 & 31.617 & 46.726 & 0.177 \\
        BaggedDT$_{\text{global}}$ & 0.740 & 25.142 & 13.021 & 33.506 & 47.116 & 0.678 \\
        RandomForest$_{\text{global}}$ & 0.732 & 25.547 & 12.908 & 33.214 & 44.513 & 0.232 \\
        BayesianNN$_{\text{global}}$ & 0.678 & 28.021 & 14.689 & 37.797 & 56.389 & 0.047 \\
        SVR$_{\text{global}}$ & 0.551 & 33.065 & 13.708 & 35.272 & 45.693 & -6.204 \\
        \bottomrule
    \end{tabular}
    }
\end{table}


\subsubsubsection{Modelos con stacking}

En la Tabla \ref{tab:stack_global_c4} se muestra el resumen del rendimiento de los modelos de stacking para la predicción de la variable de carbono en tC/ha con el conjunto de datos que emplea IFN2 e IFN3 como explicativos.

{
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \begin{longtable}{p{4.5cm}rrrrrr}
    \caption{Modelos Stacking Globales (c4)}
    \label{tab:stack_global_c4} \\
    \toprule
    Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC/ha)} & \thead{MAE$_{\text{test}}$ \\ (tC/ha)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC/ha)} \\
    \midrule
    \endfirsthead
    \toprule
    Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC/ha)} & \thead{MAE$_{\text{test}}$ \\ (tC/ha)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC/ha)} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{7}{r}{{Continúa en la siguiente página...}} \\
    \endfoot
    \bottomrule
    \endlastfoot
    \textbf{Stack5 MLP$_{\text{global}}$} & \textbf{0.794} & \textbf{22.387} & \textbf{11.321} & \textbf{29.132} & \textbf{40.529} & \textbf{0.238} \\
    Stack2 MLP$_{\text{global}}$ & 0.794 & 22.405 & 11.403 & 29.341 & 41.069 & 0.176 \\
    Stack4 MLP$_{\text{global}}$ & 0.792 & 22.527 & 11.314 & 29.112 & 40.574 & -0.045 \\
    Stack2 LinearRegression$_{\text{global}}$ & 0.791 & 22.565 & 11.429 & 29.409 & 41.712 & 0.069 \\
    Stack2 Ridge$_{\text{global}}$ & 0.791 & 22.566 & 11.429 & 29.409 & 41.696 & 0.069 \\
    Stack5 LinearRegression$_{\text{global}}$ & 0.791 & 22.571 & 11.387 & 29.299 & 41.921 & 0.057 \\
    Stack5 Ridge$_{\text{global}}$ & 0.791 & 22.572 & 11.384 & 29.293 & 41.871 & 0.058 \\
    Stack4 Ridge$_{\text{global}}$ & 0.790 & 22.601 & 11.414 & 29.371 & 41.858 & 0.083 \\
    Stack4 LinearRegression$_{\text{global}}$ & 0.790 & 22.602 & 11.416 & 29.375 & 41.905 & 0.081 \\
    Stack1 MLP$_{\text{global}}$ & 0.790 & 22.635 & 11.584 & 29.808 & 41.096 & 0.507 \\
    Stack3 LinearRegression$_{\text{global}}$ & 0.788 & 22.735 & 11.456 & 29.477 & 42.051 & 0.072 \\
    Stack3 Ridge$_{\text{global}}$ & 0.788 & 22.735 & 11.453 & 29.471 & 41.991 & 0.074 \\
    Stack1 LinearRegression$_{\text{global}}$ & 0.787 & 22.769 & 11.606 & 29.864 & 41.939 & 0.145 \\
    Stack1 Ridge$_{\text{global}}$ & 0.787 & 22.770 & 11.606 & 29.864 & 41.933 & 0.145 \\
    Stack3 MLP$_{\text{global}}$ & 0.787 & 22.776 & 11.338 & 29.175 & 40.679 & -0.384 \\
    Stack2 SVR$_{\text{global}}$ & 0.786 & 22.853 & 11.214 & 28.856 & 42.299 & -2.241 \\
    Stack5 SVR$_{\text{global}}$ & 0.785 & 22.864 & 11.162 & 28.721 & 42.381 & -2.258 \\
    Stack4 SVR$_{\text{global}}$ & 0.785 & 22.875 & 11.183 & 28.775 & 42.403 & -2.252 \\
    Stack3 SVR$_{\text{global}}$ & 0.783 & 22.996 & 11.218 & 28.866 & 42.483 & -2.263 \\
    Stack1 SVR$_{\text{global}}$ & 0.781 & 23.113 & 11.369 & 29.253 & 42.029 & -2.257 \\
    Stack2 GradientBoosting$_{\text{global}}$ & 0.779 & 23.183 & 11.527 & 29.661 & 41.592 & 0.125 \\
    Stack3 GradientBoosting$_{\text{global}}$ & 0.779 & 23.211 & 11.445 & 29.450 & 41.108 & 0.072 \\
    Stack5 GradientBoosting$_{\text{global}}$ & 0.777 & 23.284 & 11.417 & 29.378 & 41.143 & 0.064 \\
    Stack1 GradientBoosting$_{\text{global}}$ & 0.776 & 23.334 & 11.630 & 29.926 & 41.572 & 0.156 \\
    Stack4 GradientBoosting$_{\text{global}}$ & 0.773 & 23.532 & 11.477 & 29.531 & 41.095 & 0.115 \\
    Stack5 RandomForest$_{\text{global}}$ & 0.761 & 24.147 & 12.027 & 30.947 & 42.068 & 0.628 \\
    Stack2 RandomForest$_{\text{global}}$ & 0.755 & 24.446 & 12.345 & 31.766 & 43.055 & 0.378 \\
    Stack4 RandomForest$_{\text{global}}$ & 0.749 & 24.730 & 12.223 & 31.452 & 42.332 & 0.614 \\
    Stack3 RandomForest$_{\text{global}}$ & 0.748 & 24.777 & 12.407 & 31.924 & 42.760 & 0.507 \\
    Stack1 RandomForest$_{\text{global}}$ & 0.745 & 24.907 & 12.885 & 33.155 & 44.082 & 0.402 \\
    \end{longtable}
    }


\subsubsection{Toneladas de carbono}

\subsubsubsection{Modelos base}

En la Tabla \ref{tab:base_global_carbono_bruto4} se muestra el resumen del rendimiento de los modelos para la predicción de la variable de carbono en toneladas con el conjunto de datos que emplea el IFN2 e IFN3 como explicativos.


\begin{table}[H]
    \centering
    \caption{Modelos Base Globales (carbono\_bruto4)}
    \label{tab:base_global_carbono_bruto4}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lrrrrrr}
        \toprule
        Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC)} & \thead{MAE$_{\text{test}}$ \\ (tC)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC)} \\
        \midrule
        \textbf{CatBoost$_{\text{global}}$} & \textbf{0.845} & \textbf{13.846} & \textbf{6.615} & \textbf{31.789} & \textbf{98.044} & \textbf{-0.150} \\
        LightGBM$_{\text{global}}$ & 0.841 & 14.006 & 6.654 & 31.977 & 98.088 & -0.150 \\
        XGBoost$_{\text{global}}$ & 0.840 & 14.054 & 6.655 & 31.980 & 97.699 & -0.148 \\
        GBDT$_{\text{global}}$ & 0.838 & 14.159 & 6.722 & 32.304 & 98.033 & -0.132 \\
        MLP$_{\text{global}}$ & 0.832 & 14.410 & 6.931 & 33.311 & 100.379 & -0.643 \\
        BaggedDT$_{\text{global}}$ & 0.821 & 14.858 & 7.282 & 34.995 & 97.651 & 0.158 \\
        RandomForest$_{\text{global}}$ & 0.819 & 14.950 & 7.135 & 34.291 & 93.561 & -0.191 \\
        BayesianNN$_{\text{global}}$ & 0.775 & 16.674 & 8.906 & 42.802 & 107.697 & -0.097 \\
        SVR$_{\text{global}}$ & 0.679 & 19.897 & 8.137 & 39.106 & 102.061 & -4.376 \\
        \bottomrule
    \end{tabular}
    }
\end{table}



\subsubsubsection{Modelos con stacking}

En la Tabla \ref{tab:stack_global_carbono_bruto4} se muestra el resumen del rendimiento de los modelos con stacking para la predicción de la variable de carbono en toneladas con el conjunto de datos que emplea el IFN2 e IFN3 como explicativos.


{
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \begin{longtable}{p{4.5cm}rrrrrr}
    \caption{Modelos Stacking Globales (carbono\_bruto4)}
    \label{tab:stack_global_carbono_bruto4} \\
    \toprule
    Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC)} & \thead{MAE$_{\text{test}}$ \\ (tC)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC)} \\
    \midrule
    \endfirsthead
    \toprule
    Modelo & \thead{$R^2_{\text{test}}$} & \thead{RMSE$_{\text{test}}$ \\ (tC)} & \thead{MAE$_{\text{test}}$ \\ (tC)} & \thead{wMAPE \\ (\%)} & \thead{SMAPE \\ (\%)} & \thead{Bias \\ (tC)} \\
    \midrule
    \endhead
    \midrule
    \multicolumn{7}{r}{{Continúa en la siguiente página...}} \\
    \endfoot
    \bottomrule
    \endlastfoot
    \textbf{Stack5 MLP$_{\text{global}}$} & \textbf{0.847} & \textbf{13.756} & \textbf{6.399} & \textbf{30.754} & \textbf{95.102} & \textbf{-0.035} \\
    Stack4 MLP$_{\text{global}}$ & 0.847 & 13.766 & 6.429 & 30.898 & 95.960 & 0.163 \\
    Stack4 LinearRegression$_{\text{global}}$ & 0.846 & 13.782 & 6.533 & 31.394 & 98.129 & -0.126 \\
    Stack4 Ridge$_{\text{global}}$ & 0.846 & 13.782 & 6.533 & 31.394 & 98.140 & -0.126 \\
    Stack3 MLP$_{\text{global}}$ & 0.846 & 13.783 & 6.362 & 30.576 & 93.763 & -0.101 \\
    Stack5 LinearRegression$_{\text{global}}$ & 0.846 & 13.785 & 6.539 & 31.426 & 98.481 & -0.131 \\
    Stack5 Ridge$_{\text{global}}$ & 0.846 & 13.785 & 6.539 & 31.426 & 98.493 & -0.131 \\
    Stack5 GradientBoosting$_{\text{global}}$ & 0.846 & 13.799 & 6.421 & 30.860 & 96.413 & -0.131 \\
    Stack3 LinearRegression$_{\text{global}}$ & 0.846 & 13.812 & 6.556 & 31.506 & 98.252 & -0.121 \\
    Stack3 Ridge$_{\text{global}}$ & 0.846 & 13.812 & 6.556 & 31.506 & 98.252 & -0.121 \\
    Stack4 GradientBoosting$_{\text{global}}$ & 0.845 & 13.850 & 6.428 & 30.891 & 96.220 & -0.126 \\
    Stack3 GradientBoosting$_{\text{global}}$ & 0.844 & 13.860 & 6.433 & 30.917 & 96.274 & -0.125 \\
    Stack2 LinearRegression$_{\text{global}}$ & 0.843 & 13.913 & 6.586 & 31.650 & 98.655 & -0.145 \\
    Stack2 Ridge$_{\text{global}}$ & 0.843 & 13.913 & 6.586 & 31.650 & 98.653 & -0.145 \\
    Stack2 MLP$_{\text{global}}$ & 0.843 & 13.913 & 6.507 & 31.270 & 96.485 & -0.016 \\
    Stack5 SVR$_{\text{global}}$ & 0.843 & 13.940 & 6.450 & 30.999 & 98.458 & -1.233 \\
    Stack4 SVR$_{\text{global}}$ & 0.843 & 13.941 & 6.451 & 31.002 & 98.467 & -1.240 \\
    Stack1 MLP$_{\text{global}}$ & 0.842 & 13.968 & 6.480 & 31.140 & 94.798 & -0.326 \\
    Stack1 Ridge$_{\text{global}}$ & 0.842 & 13.974 & 6.620 & 31.816 & 98.389 & -0.138 \\
    Stack1 LinearRegression$_{\text{global}}$ & 0.842 & 13.974 & 6.620 & 31.816 & 98.389 & -0.138 \\
    Stack2 GradientBoosting$_{\text{global}}$ & 0.842 & 13.977 & 6.524 & 31.355 & 97.165 & -0.136 \\
    Stack3 SVR$_{\text{global}}$ & 0.842 & 13.977 & 6.472 & 31.103 & 98.404 & -1.249 \\
    Stack1 GradientBoosting$_{\text{global}}$ & 0.840 & 14.037 & 6.532 & 31.393 & 96.359 & -0.123 \\
    Stack2 SVR$_{\text{global}}$ & 0.840 & 14.064 & 6.511 & 31.290 & 98.508 & -1.226 \\
    Stack1 SVR$_{\text{global}}$ & 0.838 & 14.153 & 6.533 & 31.398 & 98.503 & -1.297 \\
    Stack5 RandomForest$_{\text{global}}$ & 0.836 & 14.218 & 6.705 & 32.220 & 87.305 & 0.220 \\
    Stack4 RandomForest$_{\text{global}}$ & 0.835 & 14.266 & 6.784 & 32.603 & 87.196 & 0.157 \\
    Stack3 RandomForest$_{\text{global}}$ & 0.829 & 14.522 & 6.898 & 33.152 & 86.696 & 0.104 \\
    Stack2 RandomForest$_{\text{global}}$ & 0.826 & 14.652 & 7.017 & 33.722 & 91.605 & 0.106 \\
    Stack1 RandomForest$_{\text{global}}$ & 0.815 & 15.130 & 7.270 & 34.939 & 86.061 & 0.015 \\
    \end{longtable}
    }


% Parte hecha por Maider
% \vspace{1cm}
% \hline
% \vspace{1cm}
% \subsection{Resultados}
% \label{subsec:resultados_modelos}

% En esta sección se presentan los resultados obtenidos por los modelos descritos en la Sección~\ref{subsec:modelosevaluados}. Las tablas de resultados completas se pueden consultar en el \ref{anexo:resultados}.

% En conjunto, los resultados obtenidos a lo largo de las cuatro configuraciones de entrenamiento analizadas (IFN3 o IFN2 y 3 como explicativos / variable objetivo en tC o tC/ha) muestran un comportamiento notablemente estable y coherente entre versiones, tanto en términos de capacidad predictiva como de generalización. De forma sistemática, los modelos basados en árboles de decisión y \textit{gradient boosting} son los que alcanzan los mejores niveles de rendimiento, destacando de manera consistente CatBoost y LightGBM como las alternativas más competitivas entre los modelos individuales, independientemente del inventario empleado o de la forma en que se expresa la variable objetivo.

% Un aspecto especialmente relevante es la alta similitud entre los valores de $R^2$ obtenidos en validación cruzada y en el conjunto de test, lo que indica que los modelos presentan una buena capacidad de generalización y no muestran síntomas apreciables de sobreajuste. Esta estabilidad se observa tanto en los escenarios con mayor volumen de información (IFN2+IFN3) como en aquellos más simples (IFN3), reforzando la robustez de los enfoques basados en árboles frente a variaciones en la disponibilidad de datos.

% La incorporación de esquemas de \textit{stacking} no produce incrementos sustanciales en el coeficiente de determinación respecto a los mejores modelos individuales. No obstante, sí se aprecia una mejora sistemática en el error absoluto medio (MAE), con reducciones que oscilan aproximadamente entre 210 y 371 kg de carbono (o kg/ha), dependiendo del escenario considerado. Esta reducción, aunque moderada en términos relativos, resulta relevante desde un punto de vista práctico, ya que implica predicciones más precisas en el rango de error típico y justifica la consideración del \textit{stacking} como una estrategia complementaria.

% En cuanto a la estructura de los ensambles, los mejores resultados se obtienen cuando se combinan modelos base de alta calidad y naturaleza similar (principalmente variantes de \textit{gradient boosting}) y se emplean metamodelos con complejidad moderada, como MLP o SVR lineal. Por el contrario, los \textit{stacks} con pocos modelos base o aquellos que incorporan metamodelos excesivamente flexibles, como Random Forest en el segundo nivel, tienden a ofrecer un rendimiento inferior, probablemente debido a la baja dimensionalidad del espacio de meta-predictores o a un sobreajuste innecesario del ruido residual.

% En síntesis, los resultados confirman que los modelos individuales basados en árboles constituyen una solución sólida y eficiente, mientras que el \textit{stacking} aporta mejoras incrementales principalmente en términos de reducción del error medio.

% \begin{table}[H]
% \centering
% \scriptsize
% \caption{Comparación sintética del rendimiento de los modelos según inventarios utilizados y variable objetivo.}
% \label{tab:comparativa_global_modelos}
% \begin{tabular}{lllrcccc}
% \toprule
% \textbf{IFN} & \textbf{Variable objetivo} & \textbf{Modelo} & \textbf{Modelos} & $\boldsymbol{R^2}$ & \textbf{RMSE} & \textbf{MAE} \\
% \midrule
% 2 y 3 & tC/ha & LightGBM & 1  & 0.79 & 22.77 & 11.65 \\
% 2 y 3 & tC/ha & stack1 + MLP & 6   & 0.79 & 22.39 & 11.32 \\
% \midrule
% 2 y 3 & tC & CatBoost & 1  & 0.84 & 13.85 & 6.61 \\
% 2 y 3 & tC & stack1 + MLP & 6    & 0.85 & 13.76 & 6.40 \\
% \midrule
% 3 & tC/ha & CatBoost & 1  & 0.8598 & 17.7087 & 9.2504 \\
% 3 & tC/ha & stack1 + MLP & 6     & 0.8656 & 17.3380 & 8.8789 \\
% \midrule
% 3 & tC & LightGBM & 1  & 0.9091 & 10.6623 & 5.4774 \\
% 3 & tC & stack1 + MLP & 6   & 0.9140 & 10.3723 & 5.2515 \\
% \bottomrule
% \end{tabular}
% \end{table}

% La Tabla~\ref{tab:comparativa_global_modelos} sintetiza el rendimiento de los mejores modelos identificados en cada una de las cuatro líneas de entrenamiento consideradas, permitiendo una comparación directa entre inventarios utilizados, variable objetivo y complejidad del modelo. En todos los escenarios se observa un patrón consistente: los modelos individuales basados en \textit{gradient boosting} (LightGBM o CatBoost) ofrecen un rendimiento sólido, que se ve ligeramente mejorado mediante la incorporación de esquemas de \textit{stacking}.

% Cuando se emplean conjuntamente los inventarios IFN2 e IFN3 y se predice la variable normalizada en tC/ha, el rendimiento del modelo individual (LightGBM) y del \textit{stack} es prácticamente equivalente en términos de $R^2$, si bien el \textit{stacking} logra una reducción apreciable del MAE, pasando de 11.65 a 11.32 tC/ha. Un comportamiento análogo se observa al predecir carbono total (tC) con IFN2 e IFN3, donde CatBoost alcanza ya valores elevados de $R^2$ (0.84), y el \textit{stack} introduce una mejora moderada pero consistente tanto en $R^2$ como en los errores (RMSE y MAE).

% En los escenarios basados exclusivamente en IFN3, los niveles de rendimiento son, en general, superiores. Para la variable en tC/ha, CatBoost explica cerca del 86\% de la varianza observada, mientras que el \textit{stack} incrementa ligeramente este valor y reduce el MAE en aproximadamente 0.37 tC/ha. De forma aún más clara, al predecir carbono total (tC), LightGBM alcanza un $R^2$ superior a 0.91, y el \textit{stacking} vuelve a aportar una mejora incremental, reduciendo el error absoluto medio hasta valores en torno a 5.25 tC.

% En conjunto, estos resultados confirman que la mayor ganancia del \textit{stacking} no reside tanto en aumentos sustanciales del $R^2$, sino en una reducción sistemática del error medio, lo que se traduce en predicciones más precisas en términos absolutos. Al mismo tiempo, la tabla pone de manifiesto que los enfoques basados en árboles constituyen una base extremadamente robusta, sobre la que los ensambles apilados actúan como un refinamiento adicional más que como un cambio de paradigma.

% \subsection{Síntesis de resultados}
% \label{subsec:resultados_sintesis}

% A partir del análisis realizado, pueden resumirse las principales conclusiones en los siguientes puntos:

% \begin{itemize}
%     \item El conjunto de datos depurado muestra una variables objetivos marcadas con gran variabilidad:
%           \texttt{carbono\_bruto4} presenta menor dispersión (SD $\approx 36$ tC/ha)
%           que \texttt{c4} (SD $\approx 47$ tC/ha), lo que anticipa un problema predictivo más complejo
%           para esta última.

%     \item El análisis ANOVA confirma que el \textit{periodo} tiene un efecto estadísticamente
%           significativo sobre ambas variables de carbono, evidenciando la existencia de variaciones
%           temporales sistemáticas relevantes para su modelización.

%     \item Entre las estrategias de selección de variables evaluadas (manual, FeatureWiz y mRMR),
%           la selección manual, basada en bloques temáticos con coherencia ecológica, ofrece el mejor
%           equilibrio entre simplicidad y rendimiento, superando en precisión y error a las selecciones
%           automáticas.

%     \item Los bloques de variables más informativos son, en orden aproximado de importancia:
%           estructura de la masa forestal, características de especie, condiciones edáficas y topográficas,
%           índices de vegetación e información climática estacional. La mayor parte del poder predictivo se
%           concentra en las características estructurales y de especie.

%     \item Los modelos individuales muestran que los métodos basados en árboles y
%           \textit{gradient boosting} (CatBoost, LightGBM, XGBoost y GBDT) alcanzan el mejor rendimiento
%           global, con valores de $R^2$ de hasta $0.85$ y errores moderados (inferiores al $50\%$ de
%           la desviación típica de la variable).

%     \item CatBoost destaca como el mejor modelo individual, gracias a su capacidad para capturar
%           relaciones no lineales y manejar adecuadamente la complejidad y heterogeneidad de los datos.

%     \item Métodos como AdaBoost, KNN o BayesianNN muestran un rendimiento sustancialmente inferior,
%           lo que los descarta como candidatos eficaces para este tipo de predicción.

%     \item Las técnicas de \textit{stacking} aportan mejoras sistemáticas en el error absoluto medio de los modelos,
%           reduciendolos, en la mejor configuración, en torno a un 5\%.
    
%     \item El rendimiento del \textit{stacking} depende del meta-modelo: los modelos lineales
%           (Regresión Lineal y Ridge) ofrecen combinaciones estables y robustas; los meta-modelos Random
%           Forest tienden al sobreajuste; y los meta-modelos moderadamente no lineales (SVR y MLP)
%           proporcionan las mayores mejoras.

%     \item \textit{TODO: En este item se habla de la capacidad de predecir del modelo para un horizonte temporal de ``entre 5 y 30 años con un nivel elevado de precisión''. En la discusión se menciona que esta precisión varía mucho dependiendo de la cantidad de carbono, así que igual habría que matizar. Podría ser buena idea que en el apartado de Resultados simplemente se expusieran los resultados obtenidos en el de Discusión se entrase ya en valoraciones.}
    
%     El modelo desarrollado es capaz de predecir, a partir de las características estructurales,
%           ecológicas y ambientales de un cultivo forestal, la cantidad de carbono almacenado
%           en un horizonte temporal de entre 5 y 30 años con un nivel elevado de precisión.
%           El mejor modelo obtenido se construye mediante un metamodelo \texttt{MLP} combinando los modelos
%           \texttt{CatBoost,LightGBM,XGBoost, Random Forest, BaggedDT} y \texttt{GBDT} y alcanza un coeficiente de determinación de
%           \textbf{$R^2 = 0.85$}, junto con un error típico de \textbf{RMSE = 13.76 tC} y un error
%           medio absoluto de \textbf{MAE = 6.40 tC}.
% \end{itemize}
